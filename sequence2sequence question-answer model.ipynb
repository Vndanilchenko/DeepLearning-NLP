{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПРИМЕР ВОПРОСНО-ОТВЕТНОЙ СИСТЕМЫ НА БАЗЕ SEQUENCE TO SEQUENCE АРХИТЕКТУРЫ\n",
    "\n",
    "Данильченко Вадим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим библиотеки\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подготовим данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зададим параметры модели\n",
    "batch_size = 64  # размер батча\n",
    "epochs = 50  # количество эпох обучения\n",
    "latent_dim = 512  # укажем количество нейронов для енкодера\n",
    "num_samples = 30000  # количество примеров для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет) расскажи о себе', 'Что читаешь? Мне нравится классика . Я тоже люблю пообщаться']\n",
      "['Привет) под вкусный кофеек настроение поболтать появилось )', 'Люблю животных, просто обожаю, как и свою работу) . Я фантастику люблю']\n"
     ]
    }
   ],
   "source": [
    "# загрузим данные\n",
    "data = joblib.load('./dialogues_prepared.pkl')\n",
    "data.shape\n",
    "questions = data.participant_1.tolist()\n",
    "answers = data.participant_2.tolist()\n",
    "\n",
    "if len(questions)>num_samples:\n",
    "    questions = questions[:num_samples]\n",
    "    answers = answers[:num_samples]\n",
    "\n",
    "print(questions[:2])\n",
    "print(answers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 75382.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 30000\n",
      "Number of unique input tokens: 293\n",
      "Number of unique output tokens: 308\n",
      "Max sequence length for inputs: 100\n",
      "Max sequence length for outputs: 100\n"
     ]
    }
   ],
   "source": [
    "# подготовим последовательность токенов для каждого примера\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "max_encoder_seq_length = 100\n",
    "max_decoder_seq_length = 100\n",
    "\n",
    "# '\\t' - начало последовательности \n",
    "# '\\n' - окончание\n",
    "for line in trange(len(questions)):\n",
    "    input_text, target_text = questions[line][:max_encoder_seq_length-2], answers[line][:max_decoder_seq_length-2]\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизуем данные\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data - целевая последовательность, содержит токены старта и конца последовательности\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data - сдвиг на один токен от decoder_input_data\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for i,(k,v) in enumerate(input_token_index.items()) if i<20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '\"': 4,\n",
       " '$': 5,\n",
       " '%': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " '*': 10,\n",
       " '+': 11,\n",
       " ',': 12,\n",
       " '-': 13,\n",
       " '.': 14,\n",
       " '/': 15,\n",
       " '0': 16,\n",
       " '1': 17,\n",
       " '2': 18,\n",
       " '3': 19}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for i,(k,v) in enumerate(target_token_index.items()) if i<20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "зададим encoder-decoder архитектуру нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# опишем энкодер\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# на выходе энкодера оставляем только состояния\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# опишем декодер, на вход которого будут приходить целевая последовательность id токенов и состояния энкодера\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# компилируем модель\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 293)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 308)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1650688     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  1681408     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 308)    158004      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,490,100\n",
      "Trainable params: 3,490,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "375/375 - 79s - loss: 0.3060 - accuracy: 0.9072 - val_loss: 0.8329 - val_accuracy: 0.8134\n",
      "Epoch 2/50\n",
      "375/375 - 78s - loss: 0.3041 - accuracy: 0.9077 - val_loss: 0.8373 - val_accuracy: 0.8132\n",
      "Epoch 3/50\n",
      "375/375 - 77s - loss: 0.3026 - accuracy: 0.9081 - val_loss: 0.8399 - val_accuracy: 0.8136\n",
      "Epoch 4/50\n",
      "375/375 - 78s - loss: 0.3007 - accuracy: 0.9085 - val_loss: 0.8449 - val_accuracy: 0.8132\n",
      "Epoch 5/50\n",
      "375/375 - 78s - loss: 0.2991 - accuracy: 0.9088 - val_loss: 0.8474 - val_accuracy: 0.8142\n",
      "Epoch 6/50\n",
      "375/375 - 81s - loss: 0.2979 - accuracy: 0.9091 - val_loss: 0.8561 - val_accuracy: 0.8126\n",
      "Epoch 7/50\n",
      "375/375 - 82s - loss: 0.2961 - accuracy: 0.9098 - val_loss: 0.8585 - val_accuracy: 0.8125\n",
      "Epoch 8/50\n",
      "375/375 - 80s - loss: 0.2945 - accuracy: 0.9101 - val_loss: 0.8585 - val_accuracy: 0.8130\n",
      "Epoch 9/50\n",
      "375/375 - 83s - loss: 0.2934 - accuracy: 0.9102 - val_loss: 0.8574 - val_accuracy: 0.8136\n",
      "Epoch 10/50\n",
      "375/375 - 86s - loss: 0.2915 - accuracy: 0.9110 - val_loss: 0.8666 - val_accuracy: 0.8127\n",
      "Epoch 11/50\n",
      "375/375 - 89s - loss: 0.2914 - accuracy: 0.9110 - val_loss: 0.8692 - val_accuracy: 0.8130\n",
      "Epoch 12/50\n",
      "375/375 - 85s - loss: 0.2874 - accuracy: 0.9123 - val_loss: 0.8727 - val_accuracy: 0.8128\n",
      "Epoch 13/50\n",
      "375/375 - 79s - loss: 0.2880 - accuracy: 0.9117 - val_loss: 0.8775 - val_accuracy: 0.8128\n",
      "Epoch 14/50\n",
      "375/375 - 78s - loss: 0.2863 - accuracy: 0.9122 - val_loss: 0.8774 - val_accuracy: 0.8130\n",
      "Epoch 15/50\n",
      "375/375 - 77s - loss: 0.2855 - accuracy: 0.9125 - val_loss: 0.8822 - val_accuracy: 0.8122\n",
      "Epoch 16/50\n",
      "375/375 - 78s - loss: 0.2852 - accuracy: 0.9128 - val_loss: 0.8878 - val_accuracy: 0.8117\n",
      "Epoch 17/50\n",
      "375/375 - 78s - loss: 0.2827 - accuracy: 0.9135 - val_loss: 0.8904 - val_accuracy: 0.8121\n",
      "Epoch 18/50\n",
      "375/375 - 79s - loss: 0.2822 - accuracy: 0.9132 - val_loss: 0.8893 - val_accuracy: 0.8126\n",
      "Epoch 19/50\n",
      "375/375 - 78s - loss: 0.2808 - accuracy: 0.9138 - val_loss: 0.8944 - val_accuracy: 0.8123\n",
      "Epoch 20/50\n",
      "375/375 - 77s - loss: 0.2792 - accuracy: 0.9143 - val_loss: 0.8962 - val_accuracy: 0.8124\n",
      "Epoch 21/50\n",
      "375/375 - 78s - loss: 0.2782 - accuracy: 0.9145 - val_loss: 0.8994 - val_accuracy: 0.8118\n",
      "Epoch 22/50\n",
      "375/375 - 78s - loss: 0.2770 - accuracy: 0.9148 - val_loss: 0.9024 - val_accuracy: 0.8122\n",
      "Epoch 23/50\n",
      "375/375 - 78s - loss: 0.2761 - accuracy: 0.9152 - val_loss: 0.9015 - val_accuracy: 0.8124\n",
      "Epoch 24/50\n",
      "375/375 - 78s - loss: 0.2750 - accuracy: 0.9154 - val_loss: 0.9090 - val_accuracy: 0.8121\n",
      "Epoch 25/50\n",
      "375/375 - 78s - loss: 0.2738 - accuracy: 0.9156 - val_loss: 0.9091 - val_accuracy: 0.8125\n",
      "Epoch 26/50\n",
      "375/375 - 78s - loss: 0.2729 - accuracy: 0.9159 - val_loss: 0.9123 - val_accuracy: 0.8124\n",
      "Epoch 27/50\n",
      "375/375 - 78s - loss: 0.2718 - accuracy: 0.9161 - val_loss: 0.9152 - val_accuracy: 0.8121\n",
      "Epoch 28/50\n",
      "375/375 - 77s - loss: 0.2704 - accuracy: 0.9166 - val_loss: 0.9190 - val_accuracy: 0.8122\n",
      "Epoch 29/50\n",
      "375/375 - 78s - loss: 0.2696 - accuracy: 0.9166 - val_loss: 0.9234 - val_accuracy: 0.8116\n",
      "Epoch 30/50\n",
      "375/375 - 78s - loss: 0.2688 - accuracy: 0.9169 - val_loss: 0.9248 - val_accuracy: 0.8119\n",
      "Epoch 31/50\n",
      "375/375 - 77s - loss: 0.2682 - accuracy: 0.9171 - val_loss: 0.9248 - val_accuracy: 0.8116\n",
      "Epoch 32/50\n",
      "375/375 - 78s - loss: 0.2666 - accuracy: 0.9175 - val_loss: 0.9311 - val_accuracy: 0.8117\n",
      "Epoch 33/50\n",
      "375/375 - 78s - loss: 0.2656 - accuracy: 0.9179 - val_loss: 0.9284 - val_accuracy: 0.8125\n",
      "Epoch 34/50\n",
      "375/375 - 78s - loss: 0.2650 - accuracy: 0.9181 - val_loss: 0.9345 - val_accuracy: 0.8113\n",
      "Epoch 35/50\n",
      "375/375 - 78s - loss: 0.2642 - accuracy: 0.9183 - val_loss: 0.9361 - val_accuracy: 0.8117\n",
      "Epoch 36/50\n",
      "375/375 - 78s - loss: 0.2635 - accuracy: 0.9183 - val_loss: 0.9423 - val_accuracy: 0.8107\n",
      "Epoch 37/50\n",
      "375/375 - 78s - loss: 0.2624 - accuracy: 0.9188 - val_loss: 0.9407 - val_accuracy: 0.8121\n",
      "Epoch 38/50\n",
      "375/375 - 77s - loss: 0.2616 - accuracy: 0.9191 - val_loss: 0.9483 - val_accuracy: 0.8109\n",
      "Epoch 39/50\n",
      "375/375 - 77s - loss: 0.2606 - accuracy: 0.9193 - val_loss: 0.9450 - val_accuracy: 0.8119\n",
      "Epoch 40/50\n",
      "375/375 - 77s - loss: 0.2599 - accuracy: 0.9196 - val_loss: 0.9488 - val_accuracy: 0.8117\n",
      "Epoch 41/50\n",
      "375/375 - 78s - loss: 0.2587 - accuracy: 0.9199 - val_loss: 0.9531 - val_accuracy: 0.8113\n",
      "Epoch 42/50\n",
      "375/375 - 78s - loss: 0.2578 - accuracy: 0.9202 - val_loss: 0.9537 - val_accuracy: 0.8112\n",
      "Epoch 43/50\n",
      "375/375 - 77s - loss: 0.2573 - accuracy: 0.9202 - val_loss: 0.9589 - val_accuracy: 0.8111\n",
      "Epoch 44/50\n",
      "375/375 - 77s - loss: 0.2562 - accuracy: 0.9206 - val_loss: 0.9604 - val_accuracy: 0.8111\n",
      "Epoch 45/50\n",
      "375/375 - 77s - loss: 0.2560 - accuracy: 0.9205 - val_loss: 0.9609 - val_accuracy: 0.8115\n",
      "Epoch 46/50\n",
      "375/375 - 77s - loss: 0.2549 - accuracy: 0.9209 - val_loss: 0.9668 - val_accuracy: 0.8112\n",
      "Epoch 47/50\n",
      "375/375 - 79s - loss: 0.2541 - accuracy: 0.9211 - val_loss: 0.9674 - val_accuracy: 0.8111\n",
      "Epoch 48/50\n",
      "375/375 - 81s - loss: 0.2534 - accuracy: 0.9213 - val_loss: 0.9697 - val_accuracy: 0.8110\n",
      "Epoch 49/50\n",
      "375/375 - 79s - loss: 0.2525 - accuracy: 0.9215 - val_loss: 0.9746 - val_accuracy: 0.8106\n",
      "Epoch 50/50\n",
      "375/375 - 79s - loss: 0.2515 - accuracy: 0.9218 - val_loss: 0.9741 - val_accuracy: 0.8115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff220f6f88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# запускаем обучение\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним модель\n",
    "model.save('seq2seq_talks.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# шаги инференса модели:\n",
    "# 1. пропустим через энкодер и получим начальное состояние декодера\n",
    "# 2. запустим декодер с полученным начальным состоянием и целевым токеном \n",
    "# старта последовательности '\\t', выходом будет следующий целевой токен\n",
    "# 3. получим состояния и используем их вместе с полученным токеном для следующей итерации\n",
    "# 4. прекратим при предсказании токена конца последовательности '\\n' \n",
    "\n",
    "# задаем модели\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, \n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем словарь индекс токена в токен\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 1. выход энкодера будет начальным состоянием декодера\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # генерируем целевой начальный таргет для декодера '\\t'\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # пункты 2-4\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # 2. декодер с начальным состоянием и целевым таргетом \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 3. полученный токен\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 4. условия выхода из цикла\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # обновим целевой токен\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # обновим состояния\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Конечно)\n",
      "Decoded sentence: \n",
      "\n",
      "-\n",
      "Input sentence: Привет\n",
      "Decoded sentence: Привет\n",
      "\n",
      "-\n",
      "Input sentence: Ой, слушай, довольно таки неплохо. Гораздо лучше, чем днём . А твои как?\n",
      "Decoded sentence: А я люблю путешествовать . А ты?\n",
      "\n",
      "-\n",
      "Input sentence: В свободное от работы время я люблю готовиться всякие вкусности. В детстве мама меня учила готовит\n",
      "Decoded sentence: Я тоже люблю покушать, как говорят домосей )) расскажи про своим любимом места помоглешь и умею лю \n",
      "\n",
      "-\n",
      "Input sentence: Ох, соболезную ..\n",
      "Decoded sentence: Нет, я один живу . Можешь заняться в театр . Ау\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# получим несколько последовательностей на основе датасета\n",
    "for seq_index in range(52, 57):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для использования в модели произвольного текста\n",
    "def make_encoder_input(text):\n",
    "    encoder_input_text = np.zeros([1, max_encoder_seq_length, num_encoder_tokens])\n",
    "    phrase = text\n",
    "    for i in trange(len(phrase)):\n",
    "        if i>max_encoder_seq_length:\n",
    "            break\n",
    "        print(phrase[i])\n",
    "        if phrase[i] not in input_characters:\n",
    "            continue\n",
    "        encoder_input_text[0, i, input_characters.index(phrase[i])] = 1\n",
    "    return encoder_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у\n",
      "м\n",
      "е\n",
      "е\n",
      "ш\n",
      "ь\n",
      " \n",
      "г\n",
      "о\n",
      "в\n",
      "о\n",
      "р\n",
      "и\n",
      "т\n",
      "ь\n",
      "?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 573.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: умеешь говорить?\n",
      "Decoded sentence: Нет, я люблю путешествовать . А ты?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "input_sentence = 'умеешь говорить?'\n",
    "input_seq = make_encoder_input(input_sentence)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input sentence:', input_sentence)\n",
    "print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "Выводы: результаты довольно интересные, но еще есть что поделать для улучшения:\n",
    "1. увеличить датасет\n",
    "2. посмотреть на диалоги, видно много разных смыслов в одном предложении, по-хорошему бы их разделить\n",
    "3. поиграть с архитектурой, убрать дэльту между точностью на обучении и тесте\n",
    "4. добавить эпох обучения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
